{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import random\n","import string\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")                     # gpu availabi;ity\n","print(f\"Using device: {device}\")\n","\n","\n","df = pd.read_csv(\"/content/poems-100.csv\")                               # loading poem\n","poems = df['text'].tolist()\n","\n","\n","def clean_text(text):                                                            # cleaning  text\n","    text = text.lower()  # Lowercase the text\n","    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n","    return text\n","\n","\n","poems = [clean_text(poem) for poem in poems]                                          # tokenization\n","all_words = [word for poem in poems for word in poem.split()]\n","vocab = sorted(list(set(all_words)))\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","idx_to_word = {i: word for i, word in enumerate(vocab)}\n","vocab_size = len(vocab)\n","\n","\n","def one_hot_encode(word_idx, vocab_size):                                           # one hot encoding function\n","    one_hot = torch.zeros(vocab_size)\n","    one_hot[word_idx] = 1\n","    return one_hot\n","\n","\n","sequences = []\n","for poem in poems:                                                               ## Create sequences\n","    words = poem.split()\n","    for i in range(1, len(words)):\n","        sequences.append((words[i - 1], words[i]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8lGVLuVmIYg","outputId":"f2068dda-5aeb-4dfb-b4da-e7f94e4c016d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["class PoemDataset(Dataset):\n","    def __init__(self, sequences, word_to_idx, vocab_size):                        # preparing dataset\n","        self.sequences = sequences\n","        self.word_to_idx = word_to_idx\n","        self.vocab_size = vocab_size\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        x, y = self.sequences[idx]\n","        x_idx = self.word_to_idx[x]\n","        y_idx = self.word_to_idx[y]\n","        x_one_hot = one_hot_encode(x_idx, self.vocab_size)\n","        return x_one_hot, torch.tensor(y_idx, dtype=torch.long)\n","\n","                                                                                        # Initialize dataset and dataloader\n","dataset = PoemDataset(sequences, word_to_idx, vocab_size)\n","dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"],"metadata":{"id":"hNps7ENHmMob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class PoemGeneratorLSTM(nn.Module):                                                     # LSTM model\n","    def __init__(self, vocab_size, hidden_dim, num_layers=2):\n","        super(PoemGeneratorLSTM, self).__init__()\n","        self.lstm = nn.LSTM(vocab_size, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x, hidden):\n","        output, hidden = self.lstm(x.unsqueeze(1), hidden)\n","        output = self.linear(output[:, -1, :])\n","        return output, hidden\n","\n","# Model initialization\n","hidden_dim = 512\n","num_layers = 2\n","model = PoemGeneratorLSTM(vocab_size, hidden_dim, num_layers).to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"Br_IAjQjmRjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 210\n","for epoch in range(epochs):\n","    total_loss = 0                                                         # training the model\n","    for inputs, targets in dataloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        hidden = None\n","        outputs, hidden = model(inputs, hidden)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yk9xueiNmUfl","outputId":"23ebb4aa-68c3-422e-b887-9c8caffe5161"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 3.4892905561417495\n","Epoch 2, Loss: 3.4605012940619275\n","Epoch 3, Loss: 3.4337211396410057\n","Epoch 4, Loss: 3.4148065710314817\n","Epoch 5, Loss: 3.398494314035603\n","Epoch 6, Loss: 3.3784999316220454\n","Epoch 7, Loss: 3.36586990134086\n","Epoch 8, Loss: 3.3533110729770956\n","Epoch 9, Loss: 3.340236270983602\n","Epoch 10, Loss: 3.3334807336639245\n","Epoch 11, Loss: 3.318866882917177\n","Epoch 12, Loss: 3.3097597897979263\n","Epoch 13, Loss: 3.3049614454180465\n","Epoch 14, Loss: 3.295273131039476\n","Epoch 15, Loss: 3.2923369592953224\n","Epoch 16, Loss: 3.285185204886402\n","Epoch 17, Loss: 3.277173000296163\n","Epoch 18, Loss: 3.2710125779858523\n","Epoch 19, Loss: 3.267511255382874\n","Epoch 20, Loss: 3.262564681354582\n","Epoch 21, Loss: 3.259156341997453\n","Epoch 22, Loss: 3.2557169093986866\n","Epoch 23, Loss: 3.2543617011351906\n","Epoch 24, Loss: 3.2464275137748126\n","Epoch 25, Loss: 3.2425549437962666\n","Epoch 26, Loss: 3.239116403105345\n","Epoch 27, Loss: 3.2366106658401885\n","Epoch 28, Loss: 3.235249259928965\n","Epoch 29, Loss: 3.2290759864866425\n","Epoch 30, Loss: 3.2246456146240234\n","Epoch 31, Loss: 3.2240106664173345\n","Epoch 32, Loss: 3.2220500120844866\n","Epoch 33, Loss: 3.218457352929782\n","Epoch 34, Loss: 3.2170200977918397\n","Epoch 35, Loss: 3.2111561199544005\n","Epoch 36, Loss: 3.2105378437536367\n","Epoch 37, Loss: 3.206919016615714\n","Epoch 38, Loss: 3.2069653748230613\n","Epoch 39, Loss: 3.203459025664651\n","Epoch 40, Loss: 3.199817515407819\n","Epoch 41, Loss: 3.19866089870275\n","Epoch 42, Loss: 3.1955156351000533\n","Epoch 43, Loss: 3.1916934554440988\n","Epoch 44, Loss: 3.190304568394478\n","Epoch 45, Loss: 3.1898632568398906\n","Epoch 46, Loss: 3.1859513589137576\n","Epoch 47, Loss: 3.182767672859943\n","Epoch 48, Loss: 3.183371609356737\n","Epoch 49, Loss: 3.1794659214316257\n","Epoch 50, Loss: 3.1797370811818175\n","Epoch 51, Loss: 3.175730224718084\n","Epoch 52, Loss: 3.1746754856307273\n","Epoch 53, Loss: 3.17405302413387\n","Epoch 54, Loss: 3.1698902345074274\n","Epoch 55, Loss: 3.170859698804549\n","Epoch 56, Loss: 3.170730653822113\n","Epoch 57, Loss: 3.166773309361749\n","Epoch 58, Loss: 3.1626411472577507\n","Epoch 59, Loss: 3.1624576081883724\n","Epoch 60, Loss: 3.160756039495913\n","Epoch 61, Loss: 3.159973002468366\n","Epoch 62, Loss: 3.157249527274018\n","Epoch 63, Loss: 3.1562516306348414\n","Epoch 64, Loss: 3.153071089729744\n","Epoch 65, Loss: 3.1530442571392947\n","Epoch 66, Loss: 3.1535304790951426\n","Epoch 67, Loss: 3.14954314083633\n","Epoch 68, Loss: 3.147221657896289\n","Epoch 69, Loss: 3.1477998735991166\n","Epoch 70, Loss: 3.1454864857727998\n","Epoch 71, Loss: 3.1433037488571722\n","Epoch 72, Loss: 3.142282308074477\n","Epoch 73, Loss: 3.1409556816278963\n","Epoch 74, Loss: 3.139720488088736\n","Epoch 75, Loss: 3.13808592489964\n","Epoch 76, Loss: 3.1366394653221485\n","Epoch 77, Loss: 3.1356726742779033\n","Epoch 78, Loss: 3.135175676543478\n","Epoch 79, Loss: 3.1338784484665627\n","Epoch 80, Loss: 3.1313308508284967\n","Epoch 81, Loss: 3.1294677726963025\n","Epoch 82, Loss: 3.128936264799049\n","Epoch 83, Loss: 3.1283942924262327\n","Epoch 84, Loss: 3.1267453141780717\n","Epoch 85, Loss: 3.1255413238248675\n","Epoch 86, Loss: 3.122929886832756\n","Epoch 87, Loss: 3.123204018785546\n","Epoch 88, Loss: 3.124176988947577\n","Epoch 89, Loss: 3.1194099725219253\n","Epoch 90, Loss: 3.1192791758423644\n","Epoch 91, Loss: 3.1180482664256517\n","Epoch 92, Loss: 3.11557057360911\n","Epoch 93, Loss: 3.1149233153446967\n","Epoch 94, Loss: 3.114328296691025\n","Epoch 95, Loss: 3.1144490093764863\n","Epoch 96, Loss: 3.112068546868359\n","Epoch 97, Loss: 3.1110041623288485\n","Epoch 98, Loss: 3.109557627396262\n","Epoch 99, Loss: 3.1080452382873376\n","Epoch 100, Loss: 3.1082890639033343\n","Epoch 101, Loss: 3.105790144421276\n","Epoch 102, Loss: 3.1045238156392783\n","Epoch 103, Loss: 3.103844671051737\n","Epoch 104, Loss: 3.103099003974638\n","Epoch 105, Loss: 3.10216610048719\n","Epoch 106, Loss: 3.1003484738305445\n","Epoch 107, Loss: 3.1019082810594627\n","Epoch 108, Loss: 3.0997843050586127\n","Epoch 109, Loss: 3.098513303025399\n","Epoch 110, Loss: 3.097319197778257\n","Epoch 111, Loss: 3.096711316874608\n","Epoch 112, Loss: 3.095744662951929\n","Epoch 113, Loss: 3.093525872947021\n","Epoch 114, Loss: 3.093152561335984\n","Epoch 115, Loss: 3.091844211588252\n","Epoch 116, Loss: 3.0924985890561434\n","Epoch 117, Loss: 3.091473044509097\n","Epoch 118, Loss: 3.090049229755303\n","Epoch 119, Loss: 3.0875785622572036\n","Epoch 120, Loss: 3.0875415629055833\n","Epoch 121, Loss: 3.0862255022315783\n","Epoch 122, Loss: 3.087428494438606\n","Epoch 123, Loss: 3.0846738642361498\n","Epoch 124, Loss: 3.083804765513524\n","Epoch 125, Loss: 3.081141565747829\n","Epoch 126, Loss: 3.082437134777326\n","Epoch 127, Loss: 3.083387885068982\n","Epoch 128, Loss: 3.082688395841134\n","Epoch 129, Loss: 3.0802862903614736\n","Epoch 130, Loss: 3.079376328176785\n","Epoch 131, Loss: 3.080201640647928\n","Epoch 132, Loss: 3.0791300244899613\n","Epoch 133, Loss: 3.077731074446841\n","Epoch 134, Loss: 3.0759485657351004\n","Epoch 135, Loss: 3.0763956981619405\n","Epoch 136, Loss: 3.0741830390969707\n","Epoch 137, Loss: 3.072440997306547\n","Epoch 138, Loss: 3.0723826823456917\n","Epoch 139, Loss: 3.0728002607513587\n","Epoch 140, Loss: 3.070313716799484\n","Epoch 141, Loss: 3.0701787484124536\n","Epoch 142, Loss: 3.070370587660241\n","Epoch 143, Loss: 3.070750241452548\n","Epoch 144, Loss: 3.068281052643771\n","Epoch 145, Loss: 3.0684054824354736\n","Epoch 146, Loss: 3.0659404986880605\n","Epoch 147, Loss: 3.0667968203984395\n","Epoch 148, Loss: 3.0657416217685363\n","Epoch 149, Loss: 3.065470163068623\n","Epoch 150, Loss: 3.06627867629491\n","Epoch 151, Loss: 3.0625286929965636\n","Epoch 152, Loss: 3.063403862127986\n","Epoch 153, Loss: 3.0609627968289073\n","Epoch 154, Loss: 3.0619045119211465\n","Epoch 155, Loss: 3.0600185060748166\n","Epoch 156, Loss: 3.0622539087898373\n","Epoch 157, Loss: 3.0603641512480424\n","Epoch 158, Loss: 3.0594164460434197\n","Epoch 159, Loss: 3.0578682669703823\n","Epoch 160, Loss: 3.058227188228943\n","Epoch 161, Loss: 3.0593055579328783\n","Epoch 162, Loss: 3.056139270258691\n","Epoch 163, Loss: 3.0562809057186304\n","Epoch 164, Loss: 3.0567735513874905\n","Epoch 165, Loss: 3.054777850758844\n","Epoch 166, Loss: 3.052511052146477\n","Epoch 167, Loss: 3.055189746649154\n","Epoch 168, Loss: 3.053769214165643\n","Epoch 169, Loss: 3.0508926569489\n","Epoch 170, Loss: 3.0510352040819555\n","Epoch 171, Loss: 3.0505101174270552\n","Epoch 172, Loss: 3.0493150869181735\n","Epoch 173, Loss: 3.05055561955111\n","Epoch 174, Loss: 3.049720171202032\n","Epoch 175, Loss: 3.0490812168220165\n","Epoch 176, Loss: 3.0499652963845842\n","Epoch 177, Loss: 3.0482836510850975\n","Epoch 178, Loss: 3.047430533819248\n","Epoch 179, Loss: 3.0462181456966104\n","Epoch 180, Loss: 3.0431863594549307\n","Epoch 181, Loss: 3.0472239435027917\n","Epoch 182, Loss: 3.0461067066291454\n","Epoch 183, Loss: 3.044392153389096\n","Epoch 184, Loss: 3.0430260841092913\n","Epoch 185, Loss: 3.0440831122620735\n","Epoch 186, Loss: 3.0413272430242033\n","Epoch 187, Loss: 3.0429848895789426\n","Epoch 188, Loss: 3.0408006554440514\n","Epoch 189, Loss: 3.0427336198678288\n","Epoch 190, Loss: 3.0407431002107925\n","Epoch 191, Loss: 3.0408723428459363\n","Epoch 192, Loss: 3.039290480045457\n","Epoch 193, Loss: 3.0411544846747205\n","Epoch 194, Loss: 3.0373920381378015\n","Epoch 195, Loss: 3.0387905088731046\n","Epoch 196, Loss: 3.0385328599208377\n","Epoch 197, Loss: 3.0368023066940704\n","Epoch 198, Loss: 3.0359094044087462\n","Epoch 199, Loss: 3.0379820882965247\n","Epoch 200, Loss: 3.036000813844908\n","Epoch 201, Loss: 3.0358299529614228\n","Epoch 202, Loss: 3.036098483930598\n","Epoch 203, Loss: 3.0356054824868632\n","Epoch 204, Loss: 3.0327124373282794\n","Epoch 205, Loss: 3.033206970580501\n","Epoch 206, Loss: 3.0314078923951775\n","Epoch 207, Loss: 3.03347185979853\n","Epoch 208, Loss: 3.0317813893056287\n","Epoch 209, Loss: 3.030897974350292\n","Epoch 210, Loss: 3.032296448791583\n"]}]},{"cell_type":"code","source":["\n","\n","# Generate poem function\n","def generate_poem_lstm(model, start_word, word_to_idx, idx_to_word, device, length=50):\n","    model.eval()\n","    generated_poem = [start_word]\n","    input_idx = word_to_idx[start_word]\n","    input_one_hot = one_hot_encode(input_idx, vocab_size).unsqueeze(0).to(device)\n","    hidden = None\n","\n","    with torch.no_grad():\n","        for _ in range(length - 1):\n","            output, hidden = model(input_one_hot, hidden)\n","            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n","            predicted_word_idx = random.choices(range(len(probabilities)), weights=probabilities, k=1)[0]\n","            predicted_word = idx_to_word[predicted_word_idx]\n","            generated_poem.append(predicted_word)\n","            input_one_hot = one_hot_encode(predicted_word_idx, vocab_size).unsqueeze(0).to(device)\n","\n","    formatted_poem = \"\"\n","    line = []\n","    word_count = 0\n","    for word in generated_poem:\n","        line.append(word)\n","        word_count += 1\n","        if word_count >= 7:\n","            formatted_poem += \" \".join(line) + \"\\n\"\n","            line = []\n","            word_count = 0\n","    if line:\n","        formatted_poem += \" \".join(line)\n","    return formatted_poem\n","\n","# Generate and print a poem\n","start_word = \"sun\"\n","generated_poem = generate_poem_lstm(model, start_word, word_to_idx, idx_to_word, device)\n","print(\"\\nGenerated Poem:\\n\", generated_poem)"],"metadata":{"id":"kL3dF6fkl3ML","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d95c2dd9-0230-406a-b5f5-5430dee3b9ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generated Poem:\n"," sun there and with the calm once\n","coalmen us notgreat god for a while\n","what piping heart i will believe you\n","see the golden same little thou frigate\n","gale had long stretches as to a\n","young journey while he mile i long\n","and reachd breathe was the midst choir\n","of\n"]}]}]}