{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOiyCCb2oe3Z",
        "outputId": "272f5403-e330-434f-d03e-2f7f6643f220"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT9MBg4Qogdf",
        "outputId": "3ad42a8c-2c0c-4e4c-ec04-17ef96e92e00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_path):\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "    return pairs\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Zñáéíóúüç¿¡ ]+\", \"\", text)\n",
        "    return text.strip().split()\n"
      ],
      "metadata": {
        "id": "qn-4kYU0ojpp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, sentences, min_freq=2):\n",
        "        self.freq = Counter()\n",
        "        for sentence in sentences:\n",
        "            self.freq.update(sentence)\n",
        "\n",
        "        self.pad = '<pad>'\n",
        "        self.sos = '<sos>'\n",
        "        self.eos = '<eos>'\n",
        "        self.unk = '<unk>'\n",
        "\n",
        "        self.itos = [self.pad, self.sos, self.eos, self.unk] + [w for w, c in self.freq.items() if c >= min_freq]\n",
        "        self.stoi = {w: i for i, w in enumerate(self.itos)}\n",
        "\n",
        "    def numericalize(self, tokens):\n",
        "        return [self.stoi.get(token, self.stoi[self.unk]) for token in tokens]\n",
        "\n",
        "    def denumericalize(self, indices):\n",
        "        return [self.itos[i] for i in indices if i not in (self.stoi[self.pad],)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n"
      ],
      "metadata": {
        "id": "OFvfBV2Lol1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, trg_vocab):\n",
        "        self.data = []\n",
        "        for src, trg in pairs:\n",
        "            src_tokens = tokenize(src)\n",
        "            trg_tokens = tokenize(trg)\n",
        "            src_ids = src_vocab.numericalize(src_tokens)\n",
        "            trg_ids = [trg_vocab.stoi['<sos>']] + trg_vocab.numericalize(trg_tokens) + [trg_vocab.stoi['<eos>']]\n",
        "            self.data.append((src_ids, trg_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_max_len = max(len(x) for x in src_batch)\n",
        "    trg_max_len = max(len(x) for x in trg_batch)\n",
        "\n",
        "    src_batch_padded = [x + [src_vocab.stoi['<pad>']] * (src_max_len - len(x)) for x in src_batch]\n",
        "    trg_batch_padded = [x + [trg_vocab.stoi['<pad>']] * (trg_max_len - len(x)) for x in trg_batch]\n",
        "\n",
        "    return torch.tensor(src_batch_padded), torch.tensor(trg_batch_padded)\n"
      ],
      "metadata": {
        "id": "ZZJvmt3gonUC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, trg_pad_idx):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        trg_vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[:, t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "LCi5ivfKopQL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip=1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, trg in iterator:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(iterator)\n"
      ],
      "metadata": {
        "id": "FilNTRHrorH6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(model, dataset, src_vocab, trg_vocab, max_len=20):\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataset:\n",
        "            src_tensor = torch.tensor([src]).to(device)\n",
        "            hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "            input_token = torch.tensor([trg_vocab.stoi['<sos>']]).to(device)\n",
        "            result = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "                top1 = output.argmax(1)\n",
        "                if top1.item() == trg_vocab.stoi['<eos>']:\n",
        "                    break\n",
        "                result.append(top1.item())\n",
        "                input_token = top1\n",
        "\n",
        "            pred_tokens = trg_vocab.denumericalize(result)\n",
        "            reference_tokens = trg_vocab.denumericalize(trg[1:-1])  # Remove <sos> and <eos>\n",
        "\n",
        "            score = sentence_bleu([reference_tokens], pred_tokens, smoothing_function=smoothie, weights=(0.5, 0.5))\n",
        "            total_score += score\n",
        "            count += 1\n",
        "\n",
        "    return total_score / count\n"
      ],
      "metadata": {
        "id": "X6GbV-NFouoK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "raw_pairs = read_data(\"/content/spa.txt\")  # <-- replace with your filename\n",
        "pairs = raw_pairs  # Don't tokenize yet!\n",
        "\n",
        "train_pairs, val_pairs = train_test_split(pairs, test_size=0.1)\n",
        "\n",
        "src_vocab = Vocab([tokenize(src) for src, _ in train_pairs])\n",
        "trg_vocab = Vocab([tokenize(trg) for _, trg in train_pairs])\n",
        "\n",
        "train_dataset = TranslationDataset(train_pairs, src_vocab, trg_vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "INPUT_DIM = len(src_vocab)\n",
        "OUTPUT_DIM = len(trg_vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM)\n",
        "model = Seq2Seq(enc, dec, trg_pad_idx=trg_vocab.stoi['<pad>']).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab.stoi['<pad>'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    loss = train(model, train_loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JavuukqWowjc",
        "outputId": "d28ceac2-29a5-47cf-fa54-21c52b7ba199"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.4226\n",
            "Epoch 2, Loss: 2.8949\n",
            "Epoch 3, Loss: 2.2303\n",
            "Epoch 4, Loss: 1.8335\n",
            "Epoch 5, Loss: 1.5659\n",
            "Epoch 6, Loss: 1.3702\n",
            "Epoch 7, Loss: 1.2256\n",
            "Epoch 8, Loss: 1.1061\n",
            "Epoch 9, Loss: 1.0096\n",
            "Epoch 10, Loss: 0.9267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    loss = train(model, train_loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVUhsyfxwKIL",
        "outputId": "16a5b1c9-d5a0-4dd7-a815-7c8499c51797"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4339\n",
            "Epoch 2, Loss: 0.4267\n",
            "Epoch 3, Loss: 0.4226\n",
            "Epoch 4, Loss: 0.4191\n",
            "Epoch 5, Loss: 0.4189\n",
            "Epoch 6, Loss: 0.4167\n",
            "Epoch 7, Loss: 0.4217\n",
            "Epoch 8, Loss: 0.4155\n",
            "Epoch 9, Loss: 0.4159\n",
            "Epoch 10, Loss: 0.4022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate BLEU on a small subset\n",
        "sample_dataset = [train_dataset[i] for i in range(100)]\n",
        "bleu = evaluate_bleu(model, sample_dataset, src_vocab, trg_vocab)\n",
        "print(f\"BLEU score: {bleu * 100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rt0WGgjo5aj",
        "outputId": "a201d98d-0452-4513-b648-6982125a0cc1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 72.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, src_vocab, trg_vocab, max_len=20):\n",
        "    model.eval()\n",
        "    tokens = tokenize(sentence)\n",
        "    numericalized = src_vocab.numericalize(tokens)\n",
        "    tensor = torch.tensor([numericalized]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(tensor)\n",
        "\n",
        "    input_token = torch.tensor([trg_vocab.stoi['<sos>']]).to(device)\n",
        "    output_indices = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "        top1 = output.argmax(1)\n",
        "        if top1.item() == trg_vocab.stoi['<eos>']:\n",
        "            break\n",
        "        output_indices.append(top1.item())\n",
        "        input_token = top1\n",
        "\n",
        "    translation = trg_vocab.denumericalize(output_indices)\n",
        "    return \" \".join(translation)\n"
      ],
      "metadata": {
        "id": "G7Z1fX6Cv6T0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"hello!\"\n",
        "translation = translate_sentence(example, model, src_vocab, trg_vocab)\n",
        "print(f\"ENGLISH: {example}\")\n",
        "print(f\"SPANISH: {translation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DNBpERNv8RV",
        "outputId": "e2c95239-b71d-43ec-cf49-ea6e0ac018a2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENGLISH: hello!\n",
            "SPANISH: hola\n"
          ]
        }
      ]
    }
  ]
}